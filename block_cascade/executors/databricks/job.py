""" Data model for task running on Databricks
"""
import logging
from typing import Any, Optional

from block_cascade.executors.databricks.resource import (
    DatabricksAutoscaleConfig,
    DatabricksResource,
    DatabricksPythonLibrary
)

from pydantic import BaseModel

logger = logging.getLogger(__name__)


class DatabricksJob(BaseModel):
    """A description of a job to run on Databricks

    Attributes
    ----------
    name
        The job display name on Vertex
    resource: DatabricksResource
        The execution cluster configuration on Databricks - see the docs for `DatabricksResource`.
    storage_path: str
        The full path to the directory for assets (input, output) on AWS (includes storage_key)
    storage_key: str
        A key suffixed to the storage location to ensure a unique path for each job.
        Also used as the `idempotency_token` in the Job API request.
    cluster_policy_id: str
        Generated by default by looking up using team name
    existing_cluster_id: str
        The id of an existing cluster to use, if specified job_config is ignored.
    run_path: str
        Path to run.py bootstrapping script on AWS
    task_args: str
        Additional args to be passed to the task
    timeout_seconds: str
        The maximum time this job can run for; default is 24 hours
    """  # noqa: E501

    name: str
    resource: DatabricksResource
    storage_path: str
    storage_key: str
    run_path: str
    cluster_policy_id: Optional[str] = None
    existing_cluster_id: Optional[str] = None
    timeout_seconds: int = 86400

    def create_payload(self):
        """"""
        task = self._task_spec()
        
        # Only add cluster configuration if not using serverless
        if not self.resource.use_serverless:
            task.update({"existing_cluster_id": self.existing_cluster_id})
            task.update({"new_cluster": self._cluster_spec()})

        payload = {
            "tasks": [task],
            "run_name": self.name,
            "timeout_seconds": self.timeout_seconds,
            "idempotency_token": self.storage_key,
            "access_control_list": [
                {
                    "group_name": self.resource.group_name,
                    "permission_level": "CAN_MANAGE",
                },
            ],
        }

        # Add environments configuration for serverless at job level
        if self.resource.use_serverless:
            payload["environments"] = [
                {
                    "environment_key": "default",
                    "spec": {
                        "dependencies": self._pip_dependencies(),
                        "environment_version": self.resource.serverless_environment_version,
                    }
                }
            ]

        return payload

    def _task_spec(self):
        task_args = self.resource.task_args or {}

        if self.existing_cluster_id is None and not self.resource.use_serverless:
            if task_args.get("libraries") is None:
                task_args["libraries"] = []
            task_args["libraries"].extend(self._libraries())
        elif self.existing_cluster_id and self.resource.use_serverless:
            # Log warning if both existing_cluster_id and use_serverless are set
            # This should be caught by validation, but adding defensive check
            logger.warning(
                "Both existing_cluster_id and use_serverless are set. "
                "Serverless mode takes precedence; existing_cluster_id will be ignored."
            )

        task_spec = {
            "task_key": f"{self.name[:32]}---{self.name[-32:]}",
            "description": "A function submitted from Cascade",
            "depends_on": [],
            "spark_python_task": {
                "python_file": self.run_path,
                "parameters": [self.storage_path, self.storage_key],
            },
            **task_args,
        }

        # Add environment_key for serverless compute
        # The environment is defined at the job level in create_payload()
        if self.resource.use_serverless:
            task_spec["environment_key"] = "default"

        return task_spec

    def _libraries(self) -> list[dict[str, Any]]:
        required_libraries = ("cloudpickle", "prefect")
        for lib in required_libraries:
            if any(lib == package.name for package in self.resource.python_libraries):
                continue
            self.resource.python_libraries.append(
                DatabricksPythonLibrary(
                    name=lib
                )
            )
        return [package.model_dump() for package in self.resource.python_libraries]

    def _pip_dependencies(self) -> list[str]:
        """
        Convert python libraries to pip dependency strings for serverless environments.
        Returns a list of pip requirement specifiers.
        """
        required_libraries = ("cloudpickle", "prefect")
        for lib in required_libraries:
            if any(lib == package.name for package in self.resource.python_libraries):
                continue
            self.resource.python_libraries.append(
                DatabricksPythonLibrary(
                    name=lib
                )
            )
        
        # Convert DatabricksPythonLibrary objects to pip requirement strings
        return [str(package) for package in self.resource.python_libraries]

    def _cluster_spec(self):
        """
        Creates a cluster spec for a Databricks job from the resource object
        passed to the DatabricksJobConfig object.
        """
        if self.existing_cluster_id or self.resource.use_serverless:
            return None
        else:
            cluster_spec = {
                "spark_version": self.resource.spark_version,
                "node_type_id": self.resource.machine,
                "policy_id": self.cluster_policy_id,
                "data_security_mode": self.resource.data_security_mode,
                "single_user_name": None,
            }
            worker_count = self.resource.worker_count
            if (
                isinstance(worker_count, DatabricksAutoscaleConfig)
                or "DatabricksAutoscaleConfig" in type(worker_count).__name__
            ):
                workers = {
                    "autoscale": {
                        "min_workers": worker_count.min_workers,
                        "max_workers": worker_count.max_workers,
                    }
                }
            elif isinstance(worker_count, int):
                workers = {"num_workers": worker_count}
            else:
                raise TypeError(
                    f"Expected `worker_count` of type `DatabricksAutoscaleConfig` or "
                    f"`int` but received {type(worker_count)}"
                )

            cluster_spec.update(workers)
            if self.resource.cluster_spec_overrides is not None:
                cluster_spec.update(self.resource.cluster_spec_overrides)
            return cluster_spec